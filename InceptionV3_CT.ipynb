{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aysavs/COVID-19/blob/main/InceptionV3_CT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZlJxv6mQUG"
      },
      "source": [
        "### Veri Kümesine erişmek için Google Drive'a bağlanma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFjZxmFn3hmJ",
        "outputId": "ec7f3708-ffa6-4e40-f5c3-7ecbf659066b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/COVID-19\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/COVID-19'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "VPuaufsD3zEF",
        "outputId": "80bab783-183e-4adf-d58a-be2d7ea941ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Copy of RESNETold.ipynb'              resnet_withoutgen.h5\n",
            "'Copy of Xception CT Formated.ipynb'  'RESNSET accurate.ipynb'\n",
            "'Copy of Xceptionold.ipynb'            vgg.h5\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                                 VGG.ipynb\n",
            " \u001b[01;34mimages\u001b[0m/                              'VGG old config 70% avgpool2D.ipynb'\n",
            " inception_ct.h5                       VGG_RESNET.h5\n",
            " inceptionv3_299.h5                   'VGG RESNET SETTINGS.ipynb'\n",
            " inceptionv3_299.hdf5                  VGG_RESNSET.hdf5\n",
            " inceptionv3.h5                        vggweights.hdf5\n",
            "'InceptionV3 Real 299.ipynb'           xception1.h5\n",
            " inceptionv3_real.h5                   xception1.hdf5\n",
            " inceptionv3_real.hdf5                 xception2_rohilrg.h5\n",
            "'InceptionV3 Real.ipynb'              'Xception 93%.ipynb'\n",
            " inception_weights_ct.hdf5            'Xception CT Formated.ipynb'\n",
            " resnet2_rohilrg.h5                    xception_formatted.h5\n",
            " resnet50.h5                           xception_gen.h5\n",
            " resnet50weights.hdf5                  Xception.ipynb\n",
            " RESNET.ipynb                         'Xception optimized.ipynb'\n",
            "'RESNET rohilrg.ipynb'                 xception_weights_formatted.hdf5\n",
            " resnetweights_gen.hdf5                xception_weights.hdf5\n",
            "'RESNET withour Generator.ipynb'\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVA71MXdmiys"
      },
      "source": [
        "### Tüm kütüphane ve gereklilikleri içe aktarma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mArgDd393_54",
        "outputId": "01ae795b-50bc-4537-abf7-5cf19a377e01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "from builtins import range, input\n",
        "\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, AveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAj_8Afcm-wU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Bazı Parametrelerin Tanımlanması"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h6epi3D6WGQ"
      },
      "outputs": [],
      "source": [
        "#görüntülerin yeniden boyutlandırma\n",
        "IMAGE_SIZE = [224, 224] # veri kümesine bağlı olarak değişiklik yapılabilir\n",
        "\n",
        "# eğitim yapılandırması:\n",
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "#yolları tanımla\n",
        "covid_path = 'data/ct/CT_COVID'\n",
        "noncovid_path = 'data/ct/CT_NonCOVID'\n",
        "\n",
        "# Path.jpg veya jpeg türü görüntü almak için glob'u kullanma\n",
        "covid_files = glob(covid_path + '/*')\n",
        "noncovid_files = glob(noncovid_path + '/*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "JgTksYxUnHRD",
        "outputId": "66d4b879-713a-4494-b7ac-cca1e6fbb51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 Covid Files:  ['data/CT_COVID/2020.03.22.20034041-p18-92-1.png', 'data/CT_COVID/2020.03.25.20037721-p27-172.png', 'data/CT_COVID/2020.02.25.20027763-p15-53%1.png', 'data/CT_COVID/2020.03.08.20031658-p15-106.png', 'data/CT_COVID/Comparison-of-different-samples-for-2019-novel-cor_2020_International-Journa-p2-21%10.png']\n",
            "Total Count:  349\n",
            "First 5 NonCovid Files:  ['data/CT_NonCOVID/782.png', 'data/CT_NonCOVID/174.png', 'data/CT_NonCOVID/781.png', 'data/CT_NonCOVID/1310.png', 'data/CT_NonCOVID/663.png']\n",
            "Total Count:  397\n"
          ]
        }
      ],
      "source": [
        "# Dosya değişkenlerinin içeriğini görselleştirme\n",
        "print(\"İlk 5 Covıd Dosyası: \",covid_files[0:5])\n",
        "print(\"Toplam Sayı: \",len(covid_files))\n",
        "print(\"İlk 5 NonCovıd Dosyası:  \",noncovid_files[0:5])\n",
        "print(\"Toplam Sayı: \",len(noncovid_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3zczMgpnMl8"
      },
      "source": [
        "### Dosyalardan Resim ve Sınıf Etiketleri Getirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DmylTfn7XND"
      },
      "outputs": [],
      "source": [
        "# Dosyalardan Resim ve Sınıf Etiketleri Getirme\n",
        "covid_labels = []\n",
        "noncovid_labels = []\n",
        "\n",
        "covid_images=[]\n",
        "noncovid_images=[]\n",
        "\n",
        "for i in range(len(covid_files)):\n",
        "  image = cv2.imread(covid_files[i]) # dosya okuma\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # biçimi keras'a göre düzenleme\n",
        "  image = cv2.resize(image,(224,224)) # modele göre yeniden boyutlandırma\n",
        "  covid_images.append(image) # resim ekle\n",
        "  covid_labels.append('CT_COVID') #sınıf etiketi ekle\n",
        "for i in range(len(noncovid_files)):\n",
        "  image = cv2.imread(noncovid_files[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  noncovid_images.append(image)\n",
        "  noncovid_labels.append('CT_NonCOVID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFJRVY65nni-"
      },
      "source": [
        "### Veri kümesinden İlk 40 Görüntüyü Görselleştirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd0x1Mw-np71"
      },
      "outputs": [],
      "source": [
        "# rastgele bir resime bakma\n",
        "def plot_images(images, title):\n",
        "    nrows, ncols = 5, 8\n",
        "    figsize = [10, 6]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, facecolor=(1, 1, 1))\n",
        "\n",
        "    for i, axi in enumerate(ax.flat):\n",
        "        axi.imshow(images[i])\n",
        "        axi.set_axis_off()\n",
        "\n",
        "    plt.suptitle(title, fontsize=24)\n",
        "    plt.tight_layout(pad=0.2, rect=[0, 0, 1, 0.9])\n",
        "    plt.show()\n",
        "plot_images(covid_images, 'Pozitif COVID-19 CT Taramaları')\n",
        "plot_images(noncovid_images, 'Negatif COVID-19 CT Taramaları')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4oSc4ake4N4"
      },
      "source": [
        "### **Normalizasyon**\n",
        "#### Model, görüntüleri piksel dizisi şeklinde alır. Bu nedenle pikselleri diziye dönüştürülür ve normalleştirilir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtlBn63m7jbT"
      },
      "outputs": [],
      "source": [
        "#Diziye dönüştürme ve [0,1] aralığına normalleştirme\n",
        "covid_images = np.array(covid_images) / 255\n",
        "noncovid_images = np.array(noncovid_images) / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXltydCxfxTo"
      },
      "source": [
        "### **Eğitim ve Test İçin Veri Setinin Bölünmesi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJPR0OjX-IFv"
      },
      "outputs": [],
      "source": [
        "# Her iki görüntü türü için eğitim ve test setlerine ayırma\n",
        "covid_x_train, covid_x_test, covid_y_train, covid_y_test = train_test_split(\n",
        "    covid_images, covid_labels, test_size=0.2)\n",
        "noncovid_x_train, noncovid_x_test, noncovid_y_train, noncovid_y_test = train_test_split(\n",
        "    noncovid_images, noncovid_labels, test_size=0.2)\n",
        "\n",
        "# Her iki görüntü türü için birleştirme kümeleri\n",
        "X_train = np.concatenate((noncovid_x_train, covid_x_train), axis=0)\n",
        "X_test = np.concatenate((noncovid_x_test, covid_x_test), axis=0)\n",
        "y_train = np.concatenate((noncovid_y_train, covid_y_train), axis=0)\n",
        "y_test = np.concatenate((noncovid_y_test, covid_y_test), axis=0)\n",
        "\n",
        "#  Etiketleri kategorilere ayırma  model için 0 veya 1\n",
        "y_train = LabelBinarizer().fit_transform(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "y_test = LabelBinarizer().fit_transform(y_test)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzwF8fPsgRR5"
      },
      "source": [
        "### Eğitim ve Test setlerinden birkaç görüntüyü görselleştirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZmqySfO1PUy"
      },
      "outputs": [],
      "source": [
        "plot_images(covid_x_train, 'X_egitim')\n",
        "plot_images(covid_x_test, 'X_test')\n",
        " # y_train and_test, X_train ve X_test için COVID ve noncovıd'yi temsil eden 0 ve 1 sınıf etiketlerini içerir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T3wkds0N5hK"
      },
      "source": [
        "### **Modeli Oluşturma Ve Görselleştirme**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "s47Amw8c-iTl",
        "outputId": "303f84ee-a561-4a07-f8ac-48721fffa9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Modeli Oluşturma\n",
        "inception = InceptionV3(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "outputs = inception.output\n",
        "outputs = Flatten(name=\"flatten\")(outputs)\n",
        "outputs = Dropout(0.5)(outputs)\n",
        "outputs = Dense(2, activation=\"softmax\")(outputs)\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=outputs)\n",
        "\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x6z8EtY_--Ar",
        "outputId": "22d671cb-55ee-4a4a-eb94-1fc92be88320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 25, 25, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 25, 25, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 25, 25, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 51200)        0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 51200)        0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            102402      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 21,905,186\n",
            "Trainable params: 102,402\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Modeli Görselleştirme\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8_B0QdQjhCM"
      },
      "source": [
        "\n",
        "### Görüntü Büyütme\n",
        "Görüntüleri eğitmek için farklı konumlarda, açılarda, vb. çevirmel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFbdLQ7yjoOS"
      },
      "outputs": [],
      "source": [
        "train_aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdG0hOYO5oc"
      },
      "source": [
        "\n",
        "### Modeli Eğitmek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KyYMAc1IyZOL",
        "outputId": "cb39eca9-2957-489a-a287-d8a276429ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "19/18 [==============================] - 8s 417ms/step - loss: 3.5650 - accuracy: 0.5705 - val_loss: 0.9084 - val_accuracy: 0.7467\n",
            "Epoch 2/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.6664 - accuracy: 0.6829 - val_loss: 1.0046 - val_accuracy: 0.7533\n",
            "Epoch 3/500\n",
            "19/18 [==============================] - 6s 316ms/step - loss: 1.4779 - accuracy: 0.7148 - val_loss: 0.8212 - val_accuracy: 0.7600\n",
            "Epoch 4/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.2434 - accuracy: 0.7349 - val_loss: 0.7627 - val_accuracy: 0.8067\n",
            "Epoch 5/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.4166 - accuracy: 0.7450 - val_loss: 0.8440 - val_accuracy: 0.7667\n",
            "Epoch 6/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 2.2820 - accuracy: 0.6829 - val_loss: 1.6615 - val_accuracy: 0.7200\n",
            "Epoch 7/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.3284 - accuracy: 0.7517 - val_loss: 1.1268 - val_accuracy: 0.7333\n",
            "Epoch 8/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.2377 - accuracy: 0.7701 - val_loss: 1.7554 - val_accuracy: 0.7000\n",
            "Epoch 9/500\n",
            "19/18 [==============================] - 7s 353ms/step - loss: 1.3228 - accuracy: 0.7332 - val_loss: 1.1854 - val_accuracy: 0.7667\n",
            "Epoch 10/500\n",
            "19/18 [==============================] - 6s 335ms/step - loss: 1.0696 - accuracy: 0.7768 - val_loss: 1.3336 - val_accuracy: 0.7133\n",
            "Epoch 11/500\n",
            "19/18 [==============================] - 6s 333ms/step - loss: 1.2082 - accuracy: 0.7852 - val_loss: 1.2210 - val_accuracy: 0.7200\n",
            "Epoch 12/500\n",
            "19/18 [==============================] - 6s 336ms/step - loss: 1.0676 - accuracy: 0.7970 - val_loss: 1.2532 - val_accuracy: 0.7133\n",
            "Epoch 13/500\n",
            "19/18 [==============================] - 6s 338ms/step - loss: 1.3986 - accuracy: 0.7634 - val_loss: 1.5586 - val_accuracy: 0.7067\n",
            "Epoch 14/500\n",
            "19/18 [==============================] - 6s 338ms/step - loss: 1.2093 - accuracy: 0.7852 - val_loss: 2.3052 - val_accuracy: 0.6400\n",
            "Epoch 15/500\n",
            "19/18 [==============================] - 6s 338ms/step - loss: 1.4230 - accuracy: 0.7601 - val_loss: 1.2009 - val_accuracy: 0.7533\n",
            "Epoch 16/500\n",
            "19/18 [==============================] - 6s 335ms/step - loss: 0.9543 - accuracy: 0.8272 - val_loss: 2.1002 - val_accuracy: 0.6867\n",
            "Epoch 17/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.5873 - accuracy: 0.7416 - val_loss: 1.4199 - val_accuracy: 0.7333\n",
            "Epoch 18/500\n",
            "19/18 [==============================] - 6s 335ms/step - loss: 1.4807 - accuracy: 0.7903 - val_loss: 1.1576 - val_accuracy: 0.7600\n",
            "Epoch 19/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.2494 - accuracy: 0.7886 - val_loss: 1.0444 - val_accuracy: 0.8133\n",
            "Epoch 20/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.4877 - accuracy: 0.7953 - val_loss: 1.3771 - val_accuracy: 0.7600\n",
            "Epoch 21/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.5595 - accuracy: 0.7836 - val_loss: 2.4476 - val_accuracy: 0.6933\n",
            "Epoch 22/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.9125 - accuracy: 0.7634 - val_loss: 1.8589 - val_accuracy: 0.7667\n",
            "Epoch 23/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.3074 - accuracy: 0.8104 - val_loss: 1.7937 - val_accuracy: 0.7800\n",
            "Epoch 24/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.2054 - accuracy: 0.8238 - val_loss: 1.4069 - val_accuracy: 0.7867\n",
            "Epoch 25/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.2722 - accuracy: 0.8003 - val_loss: 1.6951 - val_accuracy: 0.7733\n",
            "Epoch 26/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.4821 - accuracy: 0.8037 - val_loss: 2.3280 - val_accuracy: 0.7067\n",
            "Epoch 27/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.2650 - accuracy: 0.8171 - val_loss: 1.2520 - val_accuracy: 0.8067\n",
            "Epoch 28/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.7707 - accuracy: 0.7735 - val_loss: 1.4784 - val_accuracy: 0.8200\n",
            "Epoch 29/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.2333 - accuracy: 0.8205 - val_loss: 2.0646 - val_accuracy: 0.7600\n",
            "Epoch 30/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.2125 - accuracy: 0.8473 - val_loss: 2.0242 - val_accuracy: 0.7733\n",
            "Epoch 31/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.4882 - accuracy: 0.7987 - val_loss: 1.4941 - val_accuracy: 0.7667\n",
            "Epoch 32/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.0047 - accuracy: 0.8322 - val_loss: 1.8679 - val_accuracy: 0.7333\n",
            "Epoch 33/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.5673 - accuracy: 0.7936 - val_loss: 2.2463 - val_accuracy: 0.7200\n",
            "Epoch 34/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.6176 - accuracy: 0.8003 - val_loss: 2.0625 - val_accuracy: 0.7333\n",
            "Epoch 35/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.5491 - accuracy: 0.8087 - val_loss: 2.2787 - val_accuracy: 0.7467\n",
            "Epoch 36/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.7048 - accuracy: 0.8272 - val_loss: 2.6561 - val_accuracy: 0.7133\n",
            "Epoch 37/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9666 - accuracy: 0.7617 - val_loss: 2.2792 - val_accuracy: 0.7333\n",
            "Epoch 38/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.6900 - accuracy: 0.7970 - val_loss: 2.3037 - val_accuracy: 0.7133\n",
            "Epoch 39/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.5988 - accuracy: 0.8104 - val_loss: 1.7681 - val_accuracy: 0.7667\n",
            "Epoch 40/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.5377 - accuracy: 0.8205 - val_loss: 1.6750 - val_accuracy: 0.7467\n",
            "Epoch 41/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.3802 - accuracy: 0.8221 - val_loss: 2.3132 - val_accuracy: 0.7267\n",
            "Epoch 42/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.6023 - accuracy: 0.8087 - val_loss: 2.3943 - val_accuracy: 0.7133\n",
            "Epoch 43/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.3220 - accuracy: 0.8205 - val_loss: 2.4823 - val_accuracy: 0.7467\n",
            "Epoch 44/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1842 - accuracy: 0.7718 - val_loss: 2.2437 - val_accuracy: 0.7467\n",
            "Epoch 45/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.2096 - accuracy: 0.8440 - val_loss: 2.4964 - val_accuracy: 0.7267\n",
            "Epoch 46/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.2215 - accuracy: 0.8389 - val_loss: 2.1830 - val_accuracy: 0.7267\n",
            "Epoch 47/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.3919 - accuracy: 0.8406 - val_loss: 2.0859 - val_accuracy: 0.7333\n",
            "Epoch 48/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.2585 - accuracy: 0.8372 - val_loss: 2.3110 - val_accuracy: 0.7600\n",
            "Epoch 49/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0165 - accuracy: 0.8121 - val_loss: 3.3786 - val_accuracy: 0.7067\n",
            "Epoch 50/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.9228 - accuracy: 0.7919 - val_loss: 2.4752 - val_accuracy: 0.7467\n",
            "Epoch 51/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.6488 - accuracy: 0.8272 - val_loss: 2.4019 - val_accuracy: 0.7467\n",
            "Epoch 52/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.3607 - accuracy: 0.8456 - val_loss: 2.4050 - val_accuracy: 0.7600\n",
            "Epoch 53/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.8814 - accuracy: 0.8054 - val_loss: 3.0002 - val_accuracy: 0.7200\n",
            "Epoch 54/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.0430 - accuracy: 0.8003 - val_loss: 2.4378 - val_accuracy: 0.7333\n",
            "Epoch 55/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.6076 - accuracy: 0.8221 - val_loss: 2.7816 - val_accuracy: 0.7200\n",
            "Epoch 56/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.1508 - accuracy: 0.7752 - val_loss: 3.0740 - val_accuracy: 0.7133\n",
            "Epoch 57/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.4934 - accuracy: 0.8473 - val_loss: 4.4633 - val_accuracy: 0.6467\n",
            "Epoch 58/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.3017 - accuracy: 0.7886 - val_loss: 3.4724 - val_accuracy: 0.6933\n",
            "Epoch 59/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.6698 - accuracy: 0.8490 - val_loss: 2.2658 - val_accuracy: 0.7533\n",
            "Epoch 60/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.5739 - accuracy: 0.8238 - val_loss: 2.9894 - val_accuracy: 0.7067\n",
            "Epoch 61/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.4791 - accuracy: 0.8255 - val_loss: 2.6905 - val_accuracy: 0.7200\n",
            "Epoch 62/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.6595 - accuracy: 0.8372 - val_loss: 2.3852 - val_accuracy: 0.7600\n",
            "Epoch 63/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2968 - accuracy: 0.7953 - val_loss: 4.8470 - val_accuracy: 0.6600\n",
            "Epoch 64/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3399 - accuracy: 0.7936 - val_loss: 3.0050 - val_accuracy: 0.7533\n",
            "Epoch 65/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.2475 - accuracy: 0.8339 - val_loss: 2.9671 - val_accuracy: 0.7267\n",
            "Epoch 66/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.8298 - accuracy: 0.8289 - val_loss: 3.9418 - val_accuracy: 0.6867\n",
            "Epoch 67/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.3750 - accuracy: 0.8557 - val_loss: 2.8327 - val_accuracy: 0.7133\n",
            "Epoch 68/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.8014 - accuracy: 0.8238 - val_loss: 3.3562 - val_accuracy: 0.6933\n",
            "Epoch 69/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.1523 - accuracy: 0.8674 - val_loss: 2.7340 - val_accuracy: 0.7467\n",
            "Epoch 70/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.8832 - accuracy: 0.8221 - val_loss: 4.9850 - val_accuracy: 0.6400\n",
            "Epoch 71/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.0536 - accuracy: 0.8138 - val_loss: 2.4680 - val_accuracy: 0.7800\n",
            "Epoch 72/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.1793 - accuracy: 0.8255 - val_loss: 3.1437 - val_accuracy: 0.7600\n",
            "Epoch 73/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.2146 - accuracy: 0.8171 - val_loss: 3.8806 - val_accuracy: 0.6933\n",
            "Epoch 74/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.8072 - accuracy: 0.8406 - val_loss: 2.2733 - val_accuracy: 0.7600\n",
            "Epoch 75/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.7249 - accuracy: 0.8456 - val_loss: 2.2364 - val_accuracy: 0.7667\n",
            "Epoch 76/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8545 - accuracy: 0.8356 - val_loss: 4.0649 - val_accuracy: 0.7000\n",
            "Epoch 77/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.6577 - accuracy: 0.8020 - val_loss: 2.1957 - val_accuracy: 0.7600\n",
            "Epoch 78/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.8588 - accuracy: 0.7987 - val_loss: 2.5063 - val_accuracy: 0.7600\n",
            "Epoch 79/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.6979 - accuracy: 0.8389 - val_loss: 2.5680 - val_accuracy: 0.7400\n",
            "Epoch 80/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9371 - accuracy: 0.8473 - val_loss: 2.5474 - val_accuracy: 0.7933\n",
            "Epoch 81/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9276 - accuracy: 0.8322 - val_loss: 2.6238 - val_accuracy: 0.7400\n",
            "Epoch 82/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7589 - accuracy: 0.8540 - val_loss: 2.4592 - val_accuracy: 0.7667\n",
            "Epoch 83/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.8600 - accuracy: 0.8356 - val_loss: 4.1033 - val_accuracy: 0.6867\n",
            "Epoch 84/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.7131 - accuracy: 0.8440 - val_loss: 3.0631 - val_accuracy: 0.7400\n",
            "Epoch 85/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.8504 - accuracy: 0.8272 - val_loss: 2.2149 - val_accuracy: 0.8000\n",
            "Epoch 86/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.8848 - accuracy: 0.8423 - val_loss: 3.0662 - val_accuracy: 0.7200\n",
            "Epoch 87/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.0275 - accuracy: 0.8020 - val_loss: 2.8031 - val_accuracy: 0.7467\n",
            "Epoch 88/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.7598 - accuracy: 0.8456 - val_loss: 1.8691 - val_accuracy: 0.8067\n",
            "Epoch 89/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.3278 - accuracy: 0.8020 - val_loss: 2.3291 - val_accuracy: 0.8000\n",
            "Epoch 90/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.5646 - accuracy: 0.8574 - val_loss: 2.3414 - val_accuracy: 0.7800\n",
            "Epoch 91/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6410 - accuracy: 0.8507 - val_loss: 2.5693 - val_accuracy: 0.7600\n",
            "Epoch 92/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0674 - accuracy: 0.8456 - val_loss: 2.2916 - val_accuracy: 0.7867\n",
            "Epoch 93/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.5636 - accuracy: 0.8523 - val_loss: 2.6452 - val_accuracy: 0.7533\n",
            "Epoch 94/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.7611 - accuracy: 0.8507 - val_loss: 2.2478 - val_accuracy: 0.8200\n",
            "Epoch 95/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.3174 - accuracy: 0.8674 - val_loss: 2.7037 - val_accuracy: 0.7533\n",
            "Epoch 96/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.5858 - accuracy: 0.8507 - val_loss: 2.9846 - val_accuracy: 0.7133\n",
            "Epoch 97/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.4843 - accuracy: 0.8473 - val_loss: 2.7093 - val_accuracy: 0.7267\n",
            "Epoch 98/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.4869 - accuracy: 0.8540 - val_loss: 3.4448 - val_accuracy: 0.7133\n",
            "Epoch 99/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.4605 - accuracy: 0.8138 - val_loss: 4.0466 - val_accuracy: 0.7000\n",
            "Epoch 100/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.4023 - accuracy: 0.8540 - val_loss: 2.5970 - val_accuracy: 0.7667\n",
            "Epoch 101/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 1.3368 - accuracy: 0.8691 - val_loss: 2.4989 - val_accuracy: 0.7800\n",
            "Epoch 102/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.6526 - accuracy: 0.8540 - val_loss: 2.8670 - val_accuracy: 0.7467\n",
            "Epoch 103/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0236 - accuracy: 0.8221 - val_loss: 2.4113 - val_accuracy: 0.7733\n",
            "Epoch 104/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.6093 - accuracy: 0.8574 - val_loss: 2.7918 - val_accuracy: 0.7733\n",
            "Epoch 105/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.9475 - accuracy: 0.8389 - val_loss: 3.1227 - val_accuracy: 0.7533\n",
            "Epoch 106/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.4973 - accuracy: 0.8691 - val_loss: 3.1188 - val_accuracy: 0.7133\n",
            "Epoch 107/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.4546 - accuracy: 0.8742 - val_loss: 3.3694 - val_accuracy: 0.7133\n",
            "Epoch 108/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.0202 - accuracy: 0.8070 - val_loss: 2.6630 - val_accuracy: 0.7667\n",
            "Epoch 109/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.8539 - accuracy: 0.8456 - val_loss: 2.3180 - val_accuracy: 0.7667\n",
            "Epoch 110/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.5272 - accuracy: 0.8775 - val_loss: 2.6259 - val_accuracy: 0.7667\n",
            "Epoch 111/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.3485 - accuracy: 0.8624 - val_loss: 3.6246 - val_accuracy: 0.7267\n",
            "Epoch 112/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9167 - accuracy: 0.8339 - val_loss: 3.0487 - val_accuracy: 0.7400\n",
            "Epoch 113/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.7650 - accuracy: 0.8540 - val_loss: 2.6845 - val_accuracy: 0.7400\n",
            "Epoch 114/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.5864 - accuracy: 0.8607 - val_loss: 2.9509 - val_accuracy: 0.7600\n",
            "Epoch 115/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.2143 - accuracy: 0.8725 - val_loss: 3.0186 - val_accuracy: 0.7733\n",
            "Epoch 116/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3034 - accuracy: 0.8104 - val_loss: 2.8812 - val_accuracy: 0.7800\n",
            "Epoch 117/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.4914 - accuracy: 0.8087 - val_loss: 5.6530 - val_accuracy: 0.6467\n",
            "Epoch 118/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.7796 - accuracy: 0.8389 - val_loss: 3.7430 - val_accuracy: 0.7133\n",
            "Epoch 119/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.4322 - accuracy: 0.8674 - val_loss: 3.0935 - val_accuracy: 0.7467\n",
            "Epoch 120/500\n",
            "19/18 [==============================] - 6s 333ms/step - loss: 1.6058 - accuracy: 0.8641 - val_loss: 3.0255 - val_accuracy: 0.7533\n",
            "Epoch 121/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7288 - accuracy: 0.8507 - val_loss: 3.4594 - val_accuracy: 0.7400\n",
            "Epoch 122/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.6059 - accuracy: 0.8624 - val_loss: 2.8580 - val_accuracy: 0.7267\n",
            "Epoch 123/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.6546 - accuracy: 0.8557 - val_loss: 3.0514 - val_accuracy: 0.7467\n",
            "Epoch 124/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.0440 - accuracy: 0.8389 - val_loss: 4.0449 - val_accuracy: 0.7267\n",
            "Epoch 125/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.7034 - accuracy: 0.8674 - val_loss: 3.3507 - val_accuracy: 0.7333\n",
            "Epoch 126/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.9111 - accuracy: 0.8456 - val_loss: 2.9541 - val_accuracy: 0.7267\n",
            "Epoch 127/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.4796 - accuracy: 0.8540 - val_loss: 2.9373 - val_accuracy: 0.7467\n",
            "Epoch 128/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.2215 - accuracy: 0.8742 - val_loss: 3.9850 - val_accuracy: 0.7133\n",
            "Epoch 129/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.6537 - accuracy: 0.8557 - val_loss: 2.8920 - val_accuracy: 0.7667\n",
            "Epoch 130/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.6283 - accuracy: 0.8473 - val_loss: 2.7839 - val_accuracy: 0.7600\n",
            "Epoch 131/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.6492 - accuracy: 0.8574 - val_loss: 5.3973 - val_accuracy: 0.6733\n",
            "Epoch 132/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.1692 - accuracy: 0.8406 - val_loss: 3.0230 - val_accuracy: 0.7800\n",
            "Epoch 133/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.6809 - accuracy: 0.8540 - val_loss: 3.9985 - val_accuracy: 0.7067\n",
            "Epoch 134/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.9463 - accuracy: 0.8423 - val_loss: 3.0457 - val_accuracy: 0.7800\n",
            "Epoch 135/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.3605 - accuracy: 0.8893 - val_loss: 3.5733 - val_accuracy: 0.7333\n",
            "Epoch 136/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.4230 - accuracy: 0.8826 - val_loss: 3.2661 - val_accuracy: 0.7600\n",
            "Epoch 137/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.6841 - accuracy: 0.8507 - val_loss: 2.9543 - val_accuracy: 0.7867\n",
            "Epoch 138/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.0319 - accuracy: 0.8473 - val_loss: 3.0626 - val_accuracy: 0.7933\n",
            "Epoch 139/500\n",
            "19/18 [==============================] - 6s 337ms/step - loss: 1.5107 - accuracy: 0.8624 - val_loss: 4.4716 - val_accuracy: 0.7133\n",
            "Epoch 140/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.5036 - accuracy: 0.8742 - val_loss: 3.1004 - val_accuracy: 0.7467\n",
            "Epoch 141/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.5254 - accuracy: 0.8826 - val_loss: 3.9984 - val_accuracy: 0.7000\n",
            "Epoch 142/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.8699 - accuracy: 0.8574 - val_loss: 3.0274 - val_accuracy: 0.7600\n",
            "Epoch 143/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6364 - accuracy: 0.8758 - val_loss: 3.0092 - val_accuracy: 0.7867\n",
            "Epoch 144/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.4257 - accuracy: 0.8356 - val_loss: 4.4734 - val_accuracy: 0.7333\n",
            "Epoch 145/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.7375 - accuracy: 0.8540 - val_loss: 4.3503 - val_accuracy: 0.7133\n",
            "Epoch 146/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1478 - accuracy: 0.8440 - val_loss: 3.5884 - val_accuracy: 0.7400\n",
            "Epoch 147/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.9060 - accuracy: 0.8674 - val_loss: 3.2947 - val_accuracy: 0.7867\n",
            "Epoch 148/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.6563 - accuracy: 0.8507 - val_loss: 3.2643 - val_accuracy: 0.7733\n",
            "Epoch 149/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.1387 - accuracy: 0.8574 - val_loss: 4.9590 - val_accuracy: 0.6733\n",
            "Epoch 150/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.0052 - accuracy: 0.8624 - val_loss: 3.0341 - val_accuracy: 0.8067\n",
            "Epoch 151/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1746 - accuracy: 0.8473 - val_loss: 4.2917 - val_accuracy: 0.7600\n",
            "Epoch 152/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8424 - accuracy: 0.8641 - val_loss: 3.0603 - val_accuracy: 0.7933\n",
            "Epoch 153/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.8440 - accuracy: 0.8658 - val_loss: 3.4030 - val_accuracy: 0.7333\n",
            "Epoch 154/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.4283 - accuracy: 0.8339 - val_loss: 3.5797 - val_accuracy: 0.7800\n",
            "Epoch 155/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.7751 - accuracy: 0.8305 - val_loss: 8.5267 - val_accuracy: 0.6333\n",
            "Epoch 156/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 3.4828 - accuracy: 0.7819 - val_loss: 5.4734 - val_accuracy: 0.7467\n",
            "Epoch 157/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.3988 - accuracy: 0.8339 - val_loss: 4.5530 - val_accuracy: 0.6733\n",
            "Epoch 158/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 2.1423 - accuracy: 0.8591 - val_loss: 3.4089 - val_accuracy: 0.7667\n",
            "Epoch 159/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.0018 - accuracy: 0.8624 - val_loss: 3.3179 - val_accuracy: 0.8067\n",
            "Epoch 160/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.6523 - accuracy: 0.8792 - val_loss: 3.7283 - val_accuracy: 0.7533\n",
            "Epoch 161/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.7232 - accuracy: 0.8591 - val_loss: 5.5916 - val_accuracy: 0.7200\n",
            "Epoch 162/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.4762 - accuracy: 0.8305 - val_loss: 3.0827 - val_accuracy: 0.7733\n",
            "Epoch 163/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.9505 - accuracy: 0.8607 - val_loss: 3.3732 - val_accuracy: 0.8067\n",
            "Epoch 164/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9675 - accuracy: 0.8641 - val_loss: 3.3375 - val_accuracy: 0.7533\n",
            "Epoch 165/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3116 - accuracy: 0.8356 - val_loss: 3.1429 - val_accuracy: 0.8000\n",
            "Epoch 166/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 1.9744 - accuracy: 0.8691 - val_loss: 4.1105 - val_accuracy: 0.7067\n",
            "Epoch 167/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9998 - accuracy: 0.8456 - val_loss: 4.2624 - val_accuracy: 0.7267\n",
            "Epoch 168/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.6722 - accuracy: 0.8725 - val_loss: 3.6661 - val_accuracy: 0.7800\n",
            "Epoch 169/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.8053 - accuracy: 0.8607 - val_loss: 3.9454 - val_accuracy: 0.7333\n",
            "Epoch 170/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7364 - accuracy: 0.8775 - val_loss: 4.1425 - val_accuracy: 0.7200\n",
            "Epoch 171/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.4733 - accuracy: 0.8389 - val_loss: 3.7354 - val_accuracy: 0.7400\n",
            "Epoch 172/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7045 - accuracy: 0.8674 - val_loss: 3.6378 - val_accuracy: 0.7867\n",
            "Epoch 173/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7878 - accuracy: 0.8557 - val_loss: 3.5277 - val_accuracy: 0.8000\n",
            "Epoch 174/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.7781 - accuracy: 0.8557 - val_loss: 3.4486 - val_accuracy: 0.7467\n",
            "Epoch 175/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 1.8322 - accuracy: 0.8658 - val_loss: 3.0352 - val_accuracy: 0.7933\n",
            "Epoch 176/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.9903 - accuracy: 0.8624 - val_loss: 4.1000 - val_accuracy: 0.7133\n",
            "Epoch 177/500\n",
            "19/18 [==============================] - 6s 334ms/step - loss: 2.0889 - accuracy: 0.8473 - val_loss: 3.6990 - val_accuracy: 0.7400\n",
            "Epoch 178/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.7256 - accuracy: 0.8289 - val_loss: 4.1184 - val_accuracy: 0.7267\n",
            "Epoch 179/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1115 - accuracy: 0.8473 - val_loss: 3.4902 - val_accuracy: 0.7533\n",
            "Epoch 180/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.0442 - accuracy: 0.8708 - val_loss: 3.7573 - val_accuracy: 0.7333\n",
            "Epoch 181/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.0017 - accuracy: 0.8591 - val_loss: 4.4826 - val_accuracy: 0.7533\n",
            "Epoch 182/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.1503 - accuracy: 0.8490 - val_loss: 4.8241 - val_accuracy: 0.7067\n",
            "Epoch 183/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.1707 - accuracy: 0.8591 - val_loss: 3.7960 - val_accuracy: 0.7667\n",
            "Epoch 184/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.2335 - accuracy: 0.8574 - val_loss: 4.5846 - val_accuracy: 0.6933\n",
            "Epoch 185/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.4753 - accuracy: 0.8624 - val_loss: 3.2994 - val_accuracy: 0.7600\n",
            "Epoch 186/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.7046 - accuracy: 0.8641 - val_loss: 4.1878 - val_accuracy: 0.7667\n",
            "Epoch 187/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.9464 - accuracy: 0.8591 - val_loss: 4.1889 - val_accuracy: 0.7133\n",
            "Epoch 188/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.9047 - accuracy: 0.8624 - val_loss: 4.8744 - val_accuracy: 0.7467\n",
            "Epoch 189/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.9709 - accuracy: 0.8691 - val_loss: 3.5654 - val_accuracy: 0.7200\n",
            "Epoch 190/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2955 - accuracy: 0.8742 - val_loss: 3.5561 - val_accuracy: 0.7533\n",
            "Epoch 191/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7050 - accuracy: 0.8775 - val_loss: 3.7997 - val_accuracy: 0.7467\n",
            "Epoch 192/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.5002 - accuracy: 0.8826 - val_loss: 5.1694 - val_accuracy: 0.6733\n",
            "Epoch 193/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.0560 - accuracy: 0.8624 - val_loss: 3.9832 - val_accuracy: 0.7467\n",
            "Epoch 194/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.2237 - accuracy: 0.8423 - val_loss: 3.4462 - val_accuracy: 0.7533\n",
            "Epoch 195/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.5389 - accuracy: 0.8708 - val_loss: 4.0295 - val_accuracy: 0.7200\n",
            "Epoch 196/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.5535 - accuracy: 0.8708 - val_loss: 3.6130 - val_accuracy: 0.7200\n",
            "Epoch 197/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.0073 - accuracy: 0.8842 - val_loss: 3.2443 - val_accuracy: 0.7667\n",
            "Epoch 198/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.6026 - accuracy: 0.8960 - val_loss: 3.3477 - val_accuracy: 0.7267\n",
            "Epoch 199/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.6319 - accuracy: 0.8758 - val_loss: 3.7359 - val_accuracy: 0.7267\n",
            "Epoch 200/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.3433 - accuracy: 0.8909 - val_loss: 3.6075 - val_accuracy: 0.7400\n",
            "Epoch 201/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.9902 - accuracy: 0.8658 - val_loss: 3.5818 - val_accuracy: 0.7333\n",
            "Epoch 202/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.9712 - accuracy: 0.8792 - val_loss: 3.6088 - val_accuracy: 0.7333\n",
            "Epoch 203/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7393 - accuracy: 0.8674 - val_loss: 3.5035 - val_accuracy: 0.7667\n",
            "Epoch 204/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.8580 - accuracy: 0.8725 - val_loss: 6.3413 - val_accuracy: 0.6600\n",
            "Epoch 205/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.3028 - accuracy: 0.8423 - val_loss: 4.7095 - val_accuracy: 0.7667\n",
            "Epoch 206/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.8375 - accuracy: 0.8742 - val_loss: 3.3918 - val_accuracy: 0.7333\n",
            "Epoch 207/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9984 - accuracy: 0.8557 - val_loss: 4.7079 - val_accuracy: 0.7267\n",
            "Epoch 208/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.4105 - accuracy: 0.8389 - val_loss: 3.3274 - val_accuracy: 0.7533\n",
            "Epoch 209/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1460 - accuracy: 0.8389 - val_loss: 3.6778 - val_accuracy: 0.7267\n",
            "Epoch 210/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.4784 - accuracy: 0.8674 - val_loss: 3.6268 - val_accuracy: 0.7667\n",
            "Epoch 211/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.9992 - accuracy: 0.8406 - val_loss: 3.9984 - val_accuracy: 0.7133\n",
            "Epoch 212/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.0656 - accuracy: 0.8658 - val_loss: 3.8532 - val_accuracy: 0.7133\n",
            "Epoch 213/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.3580 - accuracy: 0.8876 - val_loss: 3.6530 - val_accuracy: 0.7467\n",
            "Epoch 214/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.4232 - accuracy: 0.8993 - val_loss: 3.2781 - val_accuracy: 0.7733\n",
            "Epoch 215/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.7814 - accuracy: 0.8557 - val_loss: 3.4335 - val_accuracy: 0.7600\n",
            "Epoch 216/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.0676 - accuracy: 0.8507 - val_loss: 3.5379 - val_accuracy: 0.7600\n",
            "Epoch 217/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.8872 - accuracy: 0.8641 - val_loss: 4.3176 - val_accuracy: 0.7200\n",
            "Epoch 218/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.2647 - accuracy: 0.8557 - val_loss: 3.7325 - val_accuracy: 0.7533\n",
            "Epoch 219/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.8092 - accuracy: 0.8876 - val_loss: 3.7776 - val_accuracy: 0.7467\n",
            "Epoch 220/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.8380 - accuracy: 0.8607 - val_loss: 3.7709 - val_accuracy: 0.7400\n",
            "Epoch 221/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.6001 - accuracy: 0.8859 - val_loss: 4.2543 - val_accuracy: 0.7133\n",
            "Epoch 222/500\n",
            "19/18 [==============================] - 6s 333ms/step - loss: 1.8748 - accuracy: 0.8691 - val_loss: 5.0775 - val_accuracy: 0.7400\n",
            "Epoch 223/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.2109 - accuracy: 0.8456 - val_loss: 5.0205 - val_accuracy: 0.7067\n",
            "Epoch 224/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.9430 - accuracy: 0.8523 - val_loss: 3.5731 - val_accuracy: 0.7467\n",
            "Epoch 225/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.7195 - accuracy: 0.8691 - val_loss: 4.5896 - val_accuracy: 0.7133\n",
            "Epoch 226/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.8255 - accuracy: 0.8775 - val_loss: 3.8772 - val_accuracy: 0.7733\n",
            "Epoch 227/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.6831 - accuracy: 0.8977 - val_loss: 3.2968 - val_accuracy: 0.8000\n",
            "Epoch 228/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.2762 - accuracy: 0.8977 - val_loss: 3.5789 - val_accuracy: 0.7600\n",
            "Epoch 229/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6553 - accuracy: 0.8859 - val_loss: 3.8346 - val_accuracy: 0.7600\n",
            "Epoch 230/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.2041 - accuracy: 0.8591 - val_loss: 3.7104 - val_accuracy: 0.7733\n",
            "Epoch 231/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.0811 - accuracy: 0.8691 - val_loss: 4.5504 - val_accuracy: 0.7267\n",
            "Epoch 232/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8829 - accuracy: 0.8708 - val_loss: 3.6967 - val_accuracy: 0.7667\n",
            "Epoch 233/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.1710 - accuracy: 0.8641 - val_loss: 3.9759 - val_accuracy: 0.7400\n",
            "Epoch 234/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.2877 - accuracy: 0.8591 - val_loss: 3.5380 - val_accuracy: 0.7667\n",
            "Epoch 235/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.4310 - accuracy: 0.8423 - val_loss: 3.8043 - val_accuracy: 0.7667\n",
            "Epoch 236/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.3867 - accuracy: 0.8574 - val_loss: 4.9491 - val_accuracy: 0.7400\n",
            "Epoch 237/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.9404 - accuracy: 0.8523 - val_loss: 3.4323 - val_accuracy: 0.7600\n",
            "Epoch 238/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1022 - accuracy: 0.8591 - val_loss: 4.1901 - val_accuracy: 0.7600\n",
            "Epoch 239/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.0439 - accuracy: 0.8658 - val_loss: 3.4643 - val_accuracy: 0.8000\n",
            "Epoch 240/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.6975 - accuracy: 0.8826 - val_loss: 3.1906 - val_accuracy: 0.7733\n",
            "Epoch 241/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1570 - accuracy: 0.8507 - val_loss: 3.3236 - val_accuracy: 0.7733\n",
            "Epoch 242/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.1810 - accuracy: 0.8658 - val_loss: 3.3199 - val_accuracy: 0.7933\n",
            "Epoch 243/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6618 - accuracy: 0.8859 - val_loss: 4.2858 - val_accuracy: 0.7400\n",
            "Epoch 244/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.7891 - accuracy: 0.8742 - val_loss: 3.8901 - val_accuracy: 0.7333\n",
            "Epoch 245/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.1979 - accuracy: 0.8708 - val_loss: 3.1242 - val_accuracy: 0.8133\n",
            "Epoch 246/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.3105 - accuracy: 0.8523 - val_loss: 3.7204 - val_accuracy: 0.7800\n",
            "Epoch 247/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.8224 - accuracy: 0.8356 - val_loss: 3.5212 - val_accuracy: 0.7867\n",
            "Epoch 248/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.1251 - accuracy: 0.8507 - val_loss: 3.5537 - val_accuracy: 0.7800\n",
            "Epoch 249/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6411 - accuracy: 0.8792 - val_loss: 3.9122 - val_accuracy: 0.7733\n",
            "Epoch 250/500\n",
            "19/18 [==============================] - 6s 339ms/step - loss: 1.7518 - accuracy: 0.8826 - val_loss: 3.7299 - val_accuracy: 0.7400\n",
            "Epoch 251/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.8741 - accuracy: 0.8708 - val_loss: 3.2896 - val_accuracy: 0.7667\n",
            "Epoch 252/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.7775 - accuracy: 0.8758 - val_loss: 3.3030 - val_accuracy: 0.7667\n",
            "Epoch 253/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.0890 - accuracy: 0.8557 - val_loss: 3.6761 - val_accuracy: 0.7267\n",
            "Epoch 254/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 2.1609 - accuracy: 0.8456 - val_loss: 3.1685 - val_accuracy: 0.7800\n",
            "Epoch 255/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.7711 - accuracy: 0.8725 - val_loss: 3.2734 - val_accuracy: 0.7600\n",
            "Epoch 256/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.2326 - accuracy: 0.9027 - val_loss: 4.5201 - val_accuracy: 0.7400\n",
            "Epoch 257/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.2490 - accuracy: 0.8523 - val_loss: 3.3526 - val_accuracy: 0.7600\n",
            "Epoch 258/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.6568 - accuracy: 0.8725 - val_loss: 3.8716 - val_accuracy: 0.7867\n",
            "Epoch 259/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.3236 - accuracy: 0.8540 - val_loss: 3.9299 - val_accuracy: 0.7467\n",
            "Epoch 260/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 3.3726 - accuracy: 0.8221 - val_loss: 5.8589 - val_accuracy: 0.7000\n",
            "Epoch 261/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.5427 - accuracy: 0.8540 - val_loss: 3.0639 - val_accuracy: 0.7933\n",
            "Epoch 262/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.8364 - accuracy: 0.8842 - val_loss: 2.8942 - val_accuracy: 0.7867\n",
            "Epoch 263/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 1.9589 - accuracy: 0.8775 - val_loss: 3.5685 - val_accuracy: 0.7600\n",
            "Epoch 264/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.4543 - accuracy: 0.8557 - val_loss: 3.1198 - val_accuracy: 0.7800\n",
            "Epoch 265/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.1469 - accuracy: 0.8591 - val_loss: 3.2232 - val_accuracy: 0.7933\n",
            "Epoch 266/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.0254 - accuracy: 0.8758 - val_loss: 3.9404 - val_accuracy: 0.7267\n",
            "Epoch 267/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 1.9004 - accuracy: 0.8742 - val_loss: 4.1728 - val_accuracy: 0.7400\n",
            "Epoch 268/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8899 - accuracy: 0.8893 - val_loss: 4.4498 - val_accuracy: 0.7200\n",
            "Epoch 269/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8285 - accuracy: 0.8624 - val_loss: 3.2935 - val_accuracy: 0.7467\n",
            "Epoch 270/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.6527 - accuracy: 0.8859 - val_loss: 3.2374 - val_accuracy: 0.7667\n",
            "Epoch 271/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.6143 - accuracy: 0.8775 - val_loss: 4.2281 - val_accuracy: 0.7200\n",
            "Epoch 272/500\n",
            "19/18 [==============================] - 6s 333ms/step - loss: 2.7149 - accuracy: 0.8389 - val_loss: 3.5659 - val_accuracy: 0.7733\n",
            "Epoch 273/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 2.5139 - accuracy: 0.8591 - val_loss: 5.1048 - val_accuracy: 0.7333\n",
            "Epoch 274/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.9356 - accuracy: 0.8658 - val_loss: 3.4194 - val_accuracy: 0.7400\n",
            "Epoch 275/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.9933 - accuracy: 0.8607 - val_loss: 3.2154 - val_accuracy: 0.7800\n",
            "Epoch 276/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.0312 - accuracy: 0.8691 - val_loss: 4.1065 - val_accuracy: 0.7667\n",
            "Epoch 277/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.1152 - accuracy: 0.8674 - val_loss: 4.7777 - val_accuracy: 0.7333\n",
            "Epoch 278/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.9186 - accuracy: 0.8574 - val_loss: 3.8867 - val_accuracy: 0.7467\n",
            "Epoch 279/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.2924 - accuracy: 0.8523 - val_loss: 3.9634 - val_accuracy: 0.7467\n",
            "Epoch 280/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.0744 - accuracy: 0.8826 - val_loss: 5.9890 - val_accuracy: 0.7200\n",
            "Epoch 281/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.8923 - accuracy: 0.8826 - val_loss: 4.4423 - val_accuracy: 0.7333\n",
            "Epoch 282/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.8953 - accuracy: 0.8708 - val_loss: 4.9884 - val_accuracy: 0.7200\n",
            "Epoch 283/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.7976 - accuracy: 0.8792 - val_loss: 5.3214 - val_accuracy: 0.7067\n",
            "Epoch 284/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.8782 - accuracy: 0.8842 - val_loss: 4.1430 - val_accuracy: 0.7400\n",
            "Epoch 285/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.6152 - accuracy: 0.8826 - val_loss: 3.6691 - val_accuracy: 0.7867\n",
            "Epoch 286/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.3434 - accuracy: 0.8859 - val_loss: 3.6300 - val_accuracy: 0.7800\n",
            "Epoch 287/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1522 - accuracy: 0.8691 - val_loss: 3.9405 - val_accuracy: 0.7800\n",
            "Epoch 288/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.9242 - accuracy: 0.8859 - val_loss: 3.9744 - val_accuracy: 0.7867\n",
            "Epoch 289/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.6858 - accuracy: 0.8440 - val_loss: 5.4725 - val_accuracy: 0.7333\n",
            "Epoch 290/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.6660 - accuracy: 0.8943 - val_loss: 3.8610 - val_accuracy: 0.7867\n",
            "Epoch 291/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9205 - accuracy: 0.8809 - val_loss: 4.1482 - val_accuracy: 0.7600\n",
            "Epoch 292/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1657 - accuracy: 0.8591 - val_loss: 3.8066 - val_accuracy: 0.8067\n",
            "Epoch 293/500\n",
            "19/18 [==============================] - 6s 317ms/step - loss: 1.6295 - accuracy: 0.8758 - val_loss: 6.0734 - val_accuracy: 0.7333\n",
            "Epoch 294/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.9333 - accuracy: 0.8842 - val_loss: 3.8445 - val_accuracy: 0.8067\n",
            "Epoch 295/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.2495 - accuracy: 0.9010 - val_loss: 3.7540 - val_accuracy: 0.7933\n",
            "Epoch 296/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.0525 - accuracy: 0.8742 - val_loss: 4.2319 - val_accuracy: 0.7600\n",
            "Epoch 297/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.1581 - accuracy: 0.8792 - val_loss: 4.1357 - val_accuracy: 0.7733\n",
            "Epoch 298/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.2067 - accuracy: 0.8574 - val_loss: 4.3530 - val_accuracy: 0.7800\n",
            "Epoch 299/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1225 - accuracy: 0.8591 - val_loss: 4.2720 - val_accuracy: 0.7733\n",
            "Epoch 300/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 2.3156 - accuracy: 0.8574 - val_loss: 5.5335 - val_accuracy: 0.7067\n",
            "Epoch 301/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7763 - accuracy: 0.8993 - val_loss: 4.6921 - val_accuracy: 0.7600\n",
            "Epoch 302/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.0553 - accuracy: 0.8725 - val_loss: 4.5798 - val_accuracy: 0.7933\n",
            "Epoch 303/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9751 - accuracy: 0.8691 - val_loss: 4.3155 - val_accuracy: 0.8000\n",
            "Epoch 304/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.7282 - accuracy: 0.8624 - val_loss: 5.2087 - val_accuracy: 0.7400\n",
            "Epoch 305/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1513 - accuracy: 0.8775 - val_loss: 4.0681 - val_accuracy: 0.7733\n",
            "Epoch 306/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.4330 - accuracy: 0.8674 - val_loss: 3.9466 - val_accuracy: 0.7467\n",
            "Epoch 307/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 3.6135 - accuracy: 0.8372 - val_loss: 7.0000 - val_accuracy: 0.7000\n",
            "Epoch 308/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.6242 - accuracy: 0.8507 - val_loss: 4.7195 - val_accuracy: 0.7667\n",
            "Epoch 309/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.9244 - accuracy: 0.8440 - val_loss: 4.6429 - val_accuracy: 0.7400\n",
            "Epoch 310/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1701 - accuracy: 0.8607 - val_loss: 3.8719 - val_accuracy: 0.7733\n",
            "Epoch 311/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.6992 - accuracy: 0.8775 - val_loss: 8.6777 - val_accuracy: 0.6800\n",
            "Epoch 312/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 3.0676 - accuracy: 0.8322 - val_loss: 4.0870 - val_accuracy: 0.7800\n",
            "Epoch 313/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7461 - accuracy: 0.8792 - val_loss: 4.9753 - val_accuracy: 0.7733\n",
            "Epoch 314/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.6112 - accuracy: 0.8557 - val_loss: 4.0933 - val_accuracy: 0.7600\n",
            "Epoch 315/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.6457 - accuracy: 0.8725 - val_loss: 4.6124 - val_accuracy: 0.7400\n",
            "Epoch 316/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.8013 - accuracy: 0.8557 - val_loss: 4.6133 - val_accuracy: 0.7667\n",
            "Epoch 317/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1310 - accuracy: 0.8674 - val_loss: 4.7836 - val_accuracy: 0.7400\n",
            "Epoch 318/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2664 - accuracy: 0.8674 - val_loss: 4.7280 - val_accuracy: 0.7533\n",
            "Epoch 319/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.8570 - accuracy: 0.8456 - val_loss: 5.4495 - val_accuracy: 0.7333\n",
            "Epoch 320/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.0132 - accuracy: 0.8792 - val_loss: 5.2526 - val_accuracy: 0.7400\n",
            "Epoch 321/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.5615 - accuracy: 0.8540 - val_loss: 4.0429 - val_accuracy: 0.7933\n",
            "Epoch 322/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1749 - accuracy: 0.8540 - val_loss: 5.6436 - val_accuracy: 0.7133\n",
            "Epoch 323/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.7283 - accuracy: 0.8893 - val_loss: 4.3896 - val_accuracy: 0.7333\n",
            "Epoch 324/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.1379 - accuracy: 0.8809 - val_loss: 4.3066 - val_accuracy: 0.8133\n",
            "Epoch 325/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.4945 - accuracy: 0.9027 - val_loss: 4.4889 - val_accuracy: 0.8000\n",
            "Epoch 326/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.0369 - accuracy: 0.8658 - val_loss: 4.4070 - val_accuracy: 0.7800\n",
            "Epoch 327/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1719 - accuracy: 0.8742 - val_loss: 4.2176 - val_accuracy: 0.7733\n",
            "Epoch 328/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.3696 - accuracy: 0.8641 - val_loss: 7.5574 - val_accuracy: 0.6867\n",
            "Epoch 329/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.7535 - accuracy: 0.8674 - val_loss: 4.2941 - val_accuracy: 0.7533\n",
            "Epoch 330/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9532 - accuracy: 0.8725 - val_loss: 3.6098 - val_accuracy: 0.8267\n",
            "Epoch 331/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.9944 - accuracy: 0.8809 - val_loss: 3.9095 - val_accuracy: 0.7933\n",
            "Epoch 332/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.4243 - accuracy: 0.8574 - val_loss: 3.6403 - val_accuracy: 0.7800\n",
            "Epoch 333/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.9882 - accuracy: 0.8708 - val_loss: 4.5127 - val_accuracy: 0.7400\n",
            "Epoch 334/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.1356 - accuracy: 0.8859 - val_loss: 3.9190 - val_accuracy: 0.7667\n",
            "Epoch 335/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.6480 - accuracy: 0.8926 - val_loss: 4.0516 - val_accuracy: 0.7933\n",
            "Epoch 336/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.7794 - accuracy: 0.8523 - val_loss: 4.9115 - val_accuracy: 0.7467\n",
            "Epoch 337/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8762 - accuracy: 0.8893 - val_loss: 4.6795 - val_accuracy: 0.7400\n",
            "Epoch 338/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.4736 - accuracy: 0.8490 - val_loss: 4.4682 - val_accuracy: 0.7333\n",
            "Epoch 339/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 3.3933 - accuracy: 0.8540 - val_loss: 5.6247 - val_accuracy: 0.7067\n",
            "Epoch 340/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.5555 - accuracy: 0.8993 - val_loss: 4.8880 - val_accuracy: 0.7333\n",
            "Epoch 341/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7847 - accuracy: 0.8691 - val_loss: 4.2967 - val_accuracy: 0.7667\n",
            "Epoch 342/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.2814 - accuracy: 0.8624 - val_loss: 5.2334 - val_accuracy: 0.7467\n",
            "Epoch 343/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.4085 - accuracy: 0.8523 - val_loss: 4.4668 - val_accuracy: 0.7467\n",
            "Epoch 344/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.7057 - accuracy: 0.8322 - val_loss: 5.1281 - val_accuracy: 0.7133\n",
            "Epoch 345/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9383 - accuracy: 0.8708 - val_loss: 4.7162 - val_accuracy: 0.7533\n",
            "Epoch 346/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.0917 - accuracy: 0.8809 - val_loss: 4.1768 - val_accuracy: 0.7800\n",
            "Epoch 347/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.9263 - accuracy: 0.8792 - val_loss: 4.2029 - val_accuracy: 0.7867\n",
            "Epoch 348/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.4321 - accuracy: 0.8490 - val_loss: 6.4206 - val_accuracy: 0.7200\n",
            "Epoch 349/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.6099 - accuracy: 0.8960 - val_loss: 4.5931 - val_accuracy: 0.7200\n",
            "Epoch 350/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.5480 - accuracy: 0.8826 - val_loss: 4.7398 - val_accuracy: 0.7467\n",
            "Epoch 351/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.0426 - accuracy: 0.8708 - val_loss: 4.2978 - val_accuracy: 0.7600\n",
            "Epoch 352/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.7296 - accuracy: 0.8909 - val_loss: 4.0264 - val_accuracy: 0.7800\n",
            "Epoch 353/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1161 - accuracy: 0.8658 - val_loss: 4.4294 - val_accuracy: 0.7867\n",
            "Epoch 354/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.8468 - accuracy: 0.8876 - val_loss: 4.8498 - val_accuracy: 0.8000\n",
            "Epoch 355/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.4972 - accuracy: 0.8758 - val_loss: 4.5657 - val_accuracy: 0.7667\n",
            "Epoch 356/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.6015 - accuracy: 0.8641 - val_loss: 5.4204 - val_accuracy: 0.7400\n",
            "Epoch 357/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.3036 - accuracy: 0.8742 - val_loss: 4.5870 - val_accuracy: 0.7533\n",
            "Epoch 358/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.5337 - accuracy: 0.8624 - val_loss: 4.5513 - val_accuracy: 0.7600\n",
            "Epoch 359/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.9989 - accuracy: 0.8624 - val_loss: 5.4417 - val_accuracy: 0.7467\n",
            "Epoch 360/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0583 - accuracy: 0.8574 - val_loss: 4.2891 - val_accuracy: 0.7867\n",
            "Epoch 361/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.8124 - accuracy: 0.8473 - val_loss: 4.5486 - val_accuracy: 0.7600\n",
            "Epoch 362/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1906 - accuracy: 0.8591 - val_loss: 4.1476 - val_accuracy: 0.7867\n",
            "Epoch 363/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.6650 - accuracy: 0.8859 - val_loss: 4.1279 - val_accuracy: 0.7600\n",
            "Epoch 364/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.4324 - accuracy: 0.8809 - val_loss: 4.4975 - val_accuracy: 0.7733\n",
            "Epoch 365/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.8009 - accuracy: 0.8893 - val_loss: 4.1357 - val_accuracy: 0.7600\n",
            "Epoch 366/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.8108 - accuracy: 0.8926 - val_loss: 4.1037 - val_accuracy: 0.7733\n",
            "Epoch 367/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 3.0723 - accuracy: 0.8473 - val_loss: 4.3813 - val_accuracy: 0.7667\n",
            "Epoch 368/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.5869 - accuracy: 0.8624 - val_loss: 6.2160 - val_accuracy: 0.7067\n",
            "Epoch 369/500\n",
            "19/18 [==============================] - 6s 332ms/step - loss: 3.5590 - accuracy: 0.8523 - val_loss: 5.0019 - val_accuracy: 0.7667\n",
            "Epoch 370/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.5921 - accuracy: 0.8473 - val_loss: 5.2212 - val_accuracy: 0.7333\n",
            "Epoch 371/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.6076 - accuracy: 0.8809 - val_loss: 4.4159 - val_accuracy: 0.7867\n",
            "Epoch 372/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.9127 - accuracy: 0.8725 - val_loss: 4.8636 - val_accuracy: 0.7733\n",
            "Epoch 373/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 2.2053 - accuracy: 0.8792 - val_loss: 4.8021 - val_accuracy: 0.7667\n",
            "Epoch 374/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.6822 - accuracy: 0.8909 - val_loss: 4.9263 - val_accuracy: 0.7867\n",
            "Epoch 375/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.7486 - accuracy: 0.8943 - val_loss: 4.4366 - val_accuracy: 0.7933\n",
            "Epoch 376/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.3835 - accuracy: 0.8742 - val_loss: 4.6931 - val_accuracy: 0.7667\n",
            "Epoch 377/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.3725 - accuracy: 0.8691 - val_loss: 5.6153 - val_accuracy: 0.7533\n",
            "Epoch 378/500\n",
            "19/18 [==============================] - 6s 330ms/step - loss: 1.9218 - accuracy: 0.8842 - val_loss: 4.6113 - val_accuracy: 0.8000\n",
            "Epoch 379/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.6868 - accuracy: 0.8607 - val_loss: 4.3175 - val_accuracy: 0.8067\n",
            "Epoch 380/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 3.3575 - accuracy: 0.8389 - val_loss: 4.6060 - val_accuracy: 0.7733\n",
            "Epoch 381/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 3.6077 - accuracy: 0.8406 - val_loss: 4.9777 - val_accuracy: 0.7933\n",
            "Epoch 382/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.1514 - accuracy: 0.8943 - val_loss: 3.9146 - val_accuracy: 0.8200\n",
            "Epoch 383/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.2936 - accuracy: 0.8708 - val_loss: 3.9105 - val_accuracy: 0.8133\n",
            "Epoch 384/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.2280 - accuracy: 0.8658 - val_loss: 6.4618 - val_accuracy: 0.7267\n",
            "Epoch 385/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 3.0168 - accuracy: 0.8423 - val_loss: 4.7707 - val_accuracy: 0.7867\n",
            "Epoch 386/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 2.3591 - accuracy: 0.8658 - val_loss: 5.2323 - val_accuracy: 0.7667\n",
            "Epoch 387/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.6642 - accuracy: 0.8507 - val_loss: 6.9901 - val_accuracy: 0.7333\n",
            "Epoch 388/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.7257 - accuracy: 0.8607 - val_loss: 4.2985 - val_accuracy: 0.7733\n",
            "Epoch 389/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.5242 - accuracy: 0.9094 - val_loss: 4.0870 - val_accuracy: 0.7667\n",
            "Epoch 390/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9206 - accuracy: 0.8943 - val_loss: 4.1208 - val_accuracy: 0.7867\n",
            "Epoch 391/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.2051 - accuracy: 0.8574 - val_loss: 4.0237 - val_accuracy: 0.7933\n",
            "Epoch 392/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.2904 - accuracy: 0.8658 - val_loss: 4.0612 - val_accuracy: 0.7933\n",
            "Epoch 393/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.4699 - accuracy: 0.8591 - val_loss: 4.8148 - val_accuracy: 0.7733\n",
            "Epoch 394/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 1.9798 - accuracy: 0.8775 - val_loss: 4.4005 - val_accuracy: 0.7333\n",
            "Epoch 395/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.1515 - accuracy: 0.8775 - val_loss: 3.9609 - val_accuracy: 0.7733\n",
            "Epoch 396/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.2127 - accuracy: 0.8641 - val_loss: 4.2428 - val_accuracy: 0.8000\n",
            "Epoch 397/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0531 - accuracy: 0.8876 - val_loss: 4.3971 - val_accuracy: 0.8000\n",
            "Epoch 398/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.5091 - accuracy: 0.9077 - val_loss: 4.1884 - val_accuracy: 0.8000\n",
            "Epoch 399/500\n",
            "19/18 [==============================] - 6s 317ms/step - loss: 1.8090 - accuracy: 0.8943 - val_loss: 6.3794 - val_accuracy: 0.7467\n",
            "Epoch 400/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 3.1450 - accuracy: 0.8456 - val_loss: 4.4649 - val_accuracy: 0.7733\n",
            "Epoch 401/500\n",
            "19/18 [==============================] - 6s 317ms/step - loss: 3.3787 - accuracy: 0.8490 - val_loss: 4.0919 - val_accuracy: 0.7933\n",
            "Epoch 402/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 3.2031 - accuracy: 0.8473 - val_loss: 5.2189 - val_accuracy: 0.7667\n",
            "Epoch 403/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 1.9663 - accuracy: 0.8943 - val_loss: 4.6896 - val_accuracy: 0.7867\n",
            "Epoch 404/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.5605 - accuracy: 0.8725 - val_loss: 3.7700 - val_accuracy: 0.8067\n",
            "Epoch 405/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.8467 - accuracy: 0.8758 - val_loss: 5.0227 - val_accuracy: 0.7867\n",
            "Epoch 406/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.1929 - accuracy: 0.8842 - val_loss: 5.4551 - val_accuracy: 0.7600\n",
            "Epoch 407/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.4720 - accuracy: 0.8842 - val_loss: 3.8873 - val_accuracy: 0.7800\n",
            "Epoch 408/500\n",
            "19/18 [==============================] - 6s 316ms/step - loss: 2.0933 - accuracy: 0.8893 - val_loss: 3.7631 - val_accuracy: 0.8200\n",
            "Epoch 409/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.4430 - accuracy: 0.8507 - val_loss: 5.3358 - val_accuracy: 0.7533\n",
            "Epoch 410/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.1312 - accuracy: 0.8859 - val_loss: 3.8251 - val_accuracy: 0.8000\n",
            "Epoch 411/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.2119 - accuracy: 0.8674 - val_loss: 4.2605 - val_accuracy: 0.7800\n",
            "Epoch 412/500\n",
            "19/18 [==============================] - 6s 317ms/step - loss: 2.1937 - accuracy: 0.8792 - val_loss: 3.9613 - val_accuracy: 0.8067\n",
            "Epoch 413/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 1.8082 - accuracy: 0.8893 - val_loss: 4.3890 - val_accuracy: 0.7733\n",
            "Epoch 414/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.1140 - accuracy: 0.8775 - val_loss: 4.0548 - val_accuracy: 0.7933\n",
            "Epoch 415/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.2884 - accuracy: 0.8775 - val_loss: 3.9908 - val_accuracy: 0.7867\n",
            "Epoch 416/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.5367 - accuracy: 0.8993 - val_loss: 4.4053 - val_accuracy: 0.7800\n",
            "Epoch 417/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9380 - accuracy: 0.8926 - val_loss: 4.1371 - val_accuracy: 0.8133\n",
            "Epoch 418/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9473 - accuracy: 0.8775 - val_loss: 4.9900 - val_accuracy: 0.7533\n",
            "Epoch 419/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 1.9768 - accuracy: 0.8893 - val_loss: 5.8565 - val_accuracy: 0.7333\n",
            "Epoch 420/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 2.5381 - accuracy: 0.8540 - val_loss: 4.2846 - val_accuracy: 0.7800\n",
            "Epoch 421/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.8240 - accuracy: 0.8641 - val_loss: 3.5966 - val_accuracy: 0.8200\n",
            "Epoch 422/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.6399 - accuracy: 0.8725 - val_loss: 4.8735 - val_accuracy: 0.7467\n",
            "Epoch 423/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 3.0293 - accuracy: 0.8674 - val_loss: 4.4344 - val_accuracy: 0.7467\n",
            "Epoch 424/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.0783 - accuracy: 0.8775 - val_loss: 3.7277 - val_accuracy: 0.7933\n",
            "Epoch 425/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.7338 - accuracy: 0.8658 - val_loss: 5.8538 - val_accuracy: 0.7467\n",
            "Epoch 426/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.7972 - accuracy: 0.8574 - val_loss: 3.4426 - val_accuracy: 0.8067\n",
            "Epoch 427/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3232 - accuracy: 0.8708 - val_loss: 4.1704 - val_accuracy: 0.7933\n",
            "Epoch 428/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.7307 - accuracy: 0.8708 - val_loss: 3.5077 - val_accuracy: 0.7800\n",
            "Epoch 429/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.1181 - accuracy: 0.8842 - val_loss: 3.7704 - val_accuracy: 0.7800\n",
            "Epoch 430/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9247 - accuracy: 0.8691 - val_loss: 4.8648 - val_accuracy: 0.7733\n",
            "Epoch 431/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 2.3781 - accuracy: 0.8909 - val_loss: 3.7369 - val_accuracy: 0.7733\n",
            "Epoch 432/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9949 - accuracy: 0.8859 - val_loss: 4.0072 - val_accuracy: 0.7733\n",
            "Epoch 433/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.8690 - accuracy: 0.8960 - val_loss: 3.6387 - val_accuracy: 0.8133\n",
            "Epoch 434/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.8434 - accuracy: 0.8473 - val_loss: 3.7647 - val_accuracy: 0.8000\n",
            "Epoch 435/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0305 - accuracy: 0.8909 - val_loss: 3.8690 - val_accuracy: 0.7867\n",
            "Epoch 436/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.2027 - accuracy: 0.8993 - val_loss: 3.6091 - val_accuracy: 0.8000\n",
            "Epoch 437/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.9894 - accuracy: 0.8977 - val_loss: 3.6759 - val_accuracy: 0.7867\n",
            "Epoch 438/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2791 - accuracy: 0.8607 - val_loss: 4.0186 - val_accuracy: 0.7800\n",
            "Epoch 439/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.0201 - accuracy: 0.8909 - val_loss: 4.9507 - val_accuracy: 0.7867\n",
            "Epoch 440/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 3.2736 - accuracy: 0.8456 - val_loss: 4.7389 - val_accuracy: 0.7800\n",
            "Epoch 441/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.7183 - accuracy: 0.8909 - val_loss: 5.2232 - val_accuracy: 0.7533\n",
            "Epoch 442/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.6629 - accuracy: 0.8641 - val_loss: 4.4753 - val_accuracy: 0.7800\n",
            "Epoch 443/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.7439 - accuracy: 0.8943 - val_loss: 3.7793 - val_accuracy: 0.7867\n",
            "Epoch 444/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 1.8315 - accuracy: 0.8742 - val_loss: 4.1732 - val_accuracy: 0.7867\n",
            "Epoch 445/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 1.8875 - accuracy: 0.8826 - val_loss: 3.7227 - val_accuracy: 0.7867\n",
            "Epoch 446/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.3409 - accuracy: 0.8909 - val_loss: 3.8936 - val_accuracy: 0.7800\n",
            "Epoch 447/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 2.5911 - accuracy: 0.8809 - val_loss: 3.6386 - val_accuracy: 0.8067\n",
            "Epoch 448/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.5067 - accuracy: 0.8624 - val_loss: 4.4565 - val_accuracy: 0.7800\n",
            "Epoch 449/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 2.7475 - accuracy: 0.8591 - val_loss: 6.0811 - val_accuracy: 0.7533\n",
            "Epoch 450/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.7800 - accuracy: 0.8842 - val_loss: 4.6912 - val_accuracy: 0.7933\n",
            "Epoch 451/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.9320 - accuracy: 0.8842 - val_loss: 6.3443 - val_accuracy: 0.7467\n",
            "Epoch 452/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.4330 - accuracy: 0.8691 - val_loss: 4.6432 - val_accuracy: 0.7800\n",
            "Epoch 453/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.5532 - accuracy: 0.8574 - val_loss: 5.2432 - val_accuracy: 0.7800\n",
            "Epoch 454/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2832 - accuracy: 0.8758 - val_loss: 5.7809 - val_accuracy: 0.7733\n",
            "Epoch 455/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 3.1124 - accuracy: 0.8809 - val_loss: 6.4320 - val_accuracy: 0.7400\n",
            "Epoch 456/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.6269 - accuracy: 0.8691 - val_loss: 4.6491 - val_accuracy: 0.7733\n",
            "Epoch 457/500\n",
            "19/18 [==============================] - 6s 316ms/step - loss: 2.0325 - accuracy: 0.8893 - val_loss: 5.0925 - val_accuracy: 0.7800\n",
            "Epoch 458/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.4146 - accuracy: 0.8658 - val_loss: 5.3367 - val_accuracy: 0.7800\n",
            "Epoch 459/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.3062 - accuracy: 0.8792 - val_loss: 4.7346 - val_accuracy: 0.7800\n",
            "Epoch 460/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3816 - accuracy: 0.8792 - val_loss: 4.7435 - val_accuracy: 0.7867\n",
            "Epoch 461/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.5892 - accuracy: 0.8674 - val_loss: 4.8702 - val_accuracy: 0.7933\n",
            "Epoch 462/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.1691 - accuracy: 0.8809 - val_loss: 5.4132 - val_accuracy: 0.7800\n",
            "Epoch 463/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.2496 - accuracy: 0.8826 - val_loss: 4.6576 - val_accuracy: 0.7867\n",
            "Epoch 464/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.5212 - accuracy: 0.8691 - val_loss: 4.6831 - val_accuracy: 0.7933\n",
            "Epoch 465/500\n",
            "19/18 [==============================] - 6s 328ms/step - loss: 2.1919 - accuracy: 0.8809 - val_loss: 4.8894 - val_accuracy: 0.7800\n",
            "Epoch 466/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.1568 - accuracy: 0.8926 - val_loss: 5.1620 - val_accuracy: 0.7467\n",
            "Epoch 467/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.7326 - accuracy: 0.9027 - val_loss: 4.5872 - val_accuracy: 0.7867\n",
            "Epoch 468/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.4114 - accuracy: 0.8775 - val_loss: 5.5894 - val_accuracy: 0.7600\n",
            "Epoch 469/500\n",
            "19/18 [==============================] - 6s 325ms/step - loss: 2.1803 - accuracy: 0.8826 - val_loss: 4.8224 - val_accuracy: 0.7867\n",
            "Epoch 470/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 1.8352 - accuracy: 0.9044 - val_loss: 4.7563 - val_accuracy: 0.7933\n",
            "Epoch 471/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.3330 - accuracy: 0.8775 - val_loss: 5.9390 - val_accuracy: 0.7600\n",
            "Epoch 472/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 3.0952 - accuracy: 0.8691 - val_loss: 5.7727 - val_accuracy: 0.7733\n",
            "Epoch 473/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 2.2675 - accuracy: 0.8876 - val_loss: 4.6683 - val_accuracy: 0.7667\n",
            "Epoch 474/500\n",
            "19/18 [==============================] - 6s 327ms/step - loss: 2.4614 - accuracy: 0.8725 - val_loss: 4.4677 - val_accuracy: 0.7800\n",
            "Epoch 475/500\n",
            "19/18 [==============================] - 6s 329ms/step - loss: 3.0521 - accuracy: 0.8591 - val_loss: 4.9138 - val_accuracy: 0.7733\n",
            "Epoch 476/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.5040 - accuracy: 0.8708 - val_loss: 4.4606 - val_accuracy: 0.7733\n",
            "Epoch 477/500\n",
            "19/18 [==============================] - 6s 318ms/step - loss: 2.3987 - accuracy: 0.8775 - val_loss: 4.5189 - val_accuracy: 0.7533\n",
            "Epoch 478/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.4338 - accuracy: 0.8725 - val_loss: 4.4135 - val_accuracy: 0.7600\n",
            "Epoch 479/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.0634 - accuracy: 0.8758 - val_loss: 5.4368 - val_accuracy: 0.7533\n",
            "Epoch 480/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.0388 - accuracy: 0.8859 - val_loss: 5.3312 - val_accuracy: 0.7467\n",
            "Epoch 481/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.1798 - accuracy: 0.8893 - val_loss: 4.9710 - val_accuracy: 0.7533\n",
            "Epoch 482/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.4828 - accuracy: 0.8742 - val_loss: 6.1119 - val_accuracy: 0.7467\n",
            "Epoch 483/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.4211 - accuracy: 0.8758 - val_loss: 4.6546 - val_accuracy: 0.7800\n",
            "Epoch 484/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.3640 - accuracy: 0.8926 - val_loss: 4.7186 - val_accuracy: 0.8133\n",
            "Epoch 485/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.7746 - accuracy: 0.8826 - val_loss: 5.0827 - val_accuracy: 0.7933\n",
            "Epoch 486/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 2.5989 - accuracy: 0.8641 - val_loss: 6.1745 - val_accuracy: 0.7200\n",
            "Epoch 487/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.6260 - accuracy: 0.8859 - val_loss: 4.9726 - val_accuracy: 0.7600\n",
            "Epoch 488/500\n",
            "19/18 [==============================] - 6s 326ms/step - loss: 2.1547 - accuracy: 0.9010 - val_loss: 5.8756 - val_accuracy: 0.7333\n",
            "Epoch 489/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.5606 - accuracy: 0.8758 - val_loss: 5.6534 - val_accuracy: 0.7400\n",
            "Epoch 490/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.5056 - accuracy: 0.8540 - val_loss: 6.1747 - val_accuracy: 0.7400\n",
            "Epoch 491/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 2.0545 - accuracy: 0.8943 - val_loss: 4.7335 - val_accuracy: 0.7867\n",
            "Epoch 492/500\n",
            "19/18 [==============================] - 6s 321ms/step - loss: 1.9566 - accuracy: 0.8893 - val_loss: 4.4304 - val_accuracy: 0.7933\n",
            "Epoch 493/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.2740 - accuracy: 0.8826 - val_loss: 4.2711 - val_accuracy: 0.8000\n",
            "Epoch 494/500\n",
            "19/18 [==============================] - 6s 331ms/step - loss: 1.9734 - accuracy: 0.9027 - val_loss: 4.5845 - val_accuracy: 0.7867\n",
            "Epoch 495/500\n",
            "19/18 [==============================] - 6s 320ms/step - loss: 1.3233 - accuracy: 0.9027 - val_loss: 4.4958 - val_accuracy: 0.8000\n",
            "Epoch 496/500\n",
            "19/18 [==============================] - 6s 324ms/step - loss: 2.1129 - accuracy: 0.8876 - val_loss: 4.5141 - val_accuracy: 0.7867\n",
            "Epoch 497/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.3600 - accuracy: 0.8876 - val_loss: 4.5273 - val_accuracy: 0.8067\n",
            "Epoch 498/500\n",
            "19/18 [==============================] - 6s 319ms/step - loss: 1.9746 - accuracy: 0.8960 - val_loss: 4.5149 - val_accuracy: 0.8000\n",
            "Epoch 499/500\n",
            "19/18 [==============================] - 6s 323ms/step - loss: 1.9348 - accuracy: 0.8893 - val_loss: 4.8631 - val_accuracy: 0.7800\n",
            "Epoch 500/500\n",
            "19/18 [==============================] - 6s 322ms/step - loss: 2.3724 - accuracy: 0.8725 - val_loss: 4.2568 - val_accuracy: 0.8067\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_aug.flow(X_train, y_train, batch_size=batch_size),\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    validation_steps=len(X_test) / batch_size,\n",
        "                    steps_per_epoch=len(X_train) / batch_size,\n",
        "                    epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD8kxmoDpdNh"
      },
      "outputs": [],
      "source": [
        "# Modeli ve Ağırlıkları Kaydetme\n",
        "model.save('inception_ct.h5')\n",
        "model.save_weights('inception_weights_ct.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrVVS_ebpdNn"
      },
      "outputs": [],
      "source": [
        "# Kaydedilen modeli yükleme\n",
        "model = load_model('inception_ct.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phWro75kSBI2"
      },
      "source": [
        "### **Tahminlerde Bulunma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTivXYAEpdNs"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBiA8iqGSGr7"
      },
      "source": [
        "### İlk 10 tahminin görselleştirilmesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S22ulPybSYLe"
      },
      "outputs": [],
      "source": [
        "prediction=y_pred[0:10]\n",
        "for index, probability in enumerate(prediction):\n",
        "  if probability[1] > 0.5:\n",
        "        plt.title('%.2f' % (probability[1]*100) + '% COVID')\n",
        "  else:\n",
        "        plt.title('%.2f' % ((1-probability[1])*100) + '% NonCOVID')\n",
        "  plt.style.reload_library\n",
        "  plt.imshow(X_test[index])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7OnPLZZp3tq"
      },
      "outputs": [],
      "source": [
        "#  Binary dönüştürme\n",
        "y_pred_bin = np.argmax(y_pred, axis=1)\n",
        "y_test_bin = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAPfnQ2Np3PE"
      },
      "source": [
        "###  ROC Eğrisi Çizimi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7k3UOqThPLw"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred_bin)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.title('ROC grafiğil')\n",
        "plt.xlabel('False Positif')\n",
        "plt.ylabel('True Pozitif')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNyZRJ1JfYC1"
      },
      "source": [
        "###  Karışıklık Matrisi Grafiği"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t80JK23kVxua"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(normalize):\n",
        "  classes = ['COVID','NonCOVID']\n",
        "  tick_marks = [0.5,1.5]\n",
        "  cn = confusion_matrix(y_test_bin, y_pred_bin,normalize=normalize)\n",
        "  sns.heatmap(cn,cmap='plasma',annot=True)\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  plt.title('Karışıklık Matrisi')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "print('Normalleştirme olmadan Karışıklık Matrisi')\n",
        "plot_confusion_matrix(normalize=None)\n",
        "\n",
        "print('Normalleştirilmiş Değerlerle Karışıklık Matrisi')\n",
        "plot_confusion_matrix(normalize='true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixuAjxVGLk-D"
      },
      "source": [
        "### Sınıflandırma Raporu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "kFSCY2HH7uVc",
        "outputId": "8d0a7ce2-85d8-4c7c-f3d2-72a36e87be21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.97      0.93        70\n",
            "           1       0.97      0.89      0.93        80\n",
            "\n",
            "    accuracy                           0.93       150\n",
            "   macro avg       0.93      0.93      0.93       150\n",
            "weighted avg       0.93      0.93      0.93       150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_bin, y_pred_bin))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1makQs5MMlU"
      },
      "source": [
        "\n",
        "### **Doğruluk ve Kayıp Grafiği**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26Vwg9q47_sK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Dogruluk Modeli')\n",
        "plt.ylabel('Dogruluk')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Egitim', 'Test'])\n",
        "plt.savefig('inception_ct_accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgwr4yJA8A0e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Kayıp Modeli')\n",
        "plt.ylabel('Kayıp')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Egitim', 'Test'])\n",
        "plt.savefig('inception_ct_loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCnB9YxPM-R9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}